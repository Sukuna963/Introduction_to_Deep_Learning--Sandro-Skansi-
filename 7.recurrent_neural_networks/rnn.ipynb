{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "ce0af2d0-4305-4015-9b90-d487e23b9e7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers import Dense\n",
    "from keras.layers import Activation\n",
    "from keras.layers import SimpleRNN\n",
    "from keras.models import Sequential\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "ec6a5294-22a0-4f61-b1d7-251facfde52e",
   "metadata": {},
   "outputs": [],
   "source": [
    "hidden_neurons = 50\n",
    "my_optimizer = \"sgd\"\n",
    "batch_size = 60\n",
    "error_function = \"mean_squared_error\"\n",
    "output_nonlinearity = \"softmax\"\n",
    "cycles = 5\n",
    "epochs_per_cycle = 3\n",
    "context = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "21fefc1c-cce7-48ee-b164-7213dfc44827",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_tesla_text_from_file(textfile=\"tesla.txt\"):\n",
    "    clean_text_chunks = []\n",
    "    with open(textfile, 'r', encoding='utf-8') as text:\n",
    "        for line in text:\n",
    "            clean_text_chunks.append(line)\n",
    "    clean_text = (\"\".join(clean_text_chunks)).lower()\n",
    "    text_as_list = clean_text.split()\n",
    "    return text_as_list\n",
    "\n",
    "text_as_list = create_tesla_text_from_file()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "05ce2e45-2b67-4b5e-88d4-cadf872e412c",
   "metadata": {},
   "outputs": [],
   "source": [
    "distinct_words = set(text_as_list)\n",
    "number_of_words = len(distinct_words)\n",
    "word2index = dict((w, i) for i, w in enumerate(distinct_words))\n",
    "index2word = dict((i, w) for i, w in enumerate(distinct_words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "b447df98-d328-4ea5-be87-437c0c1a1660",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_word_indices_for_text(text_as_list):\n",
    "    input_words = []\n",
    "    label_words = []\n",
    "    for i in range(0, len(text_as_list) - context):\n",
    "        input_words.append((text_as_list[i:i+context]))\n",
    "        label_words.append((text_as_list[i+context]))\n",
    "    return input_words, label_words\n",
    "input_words, label_words = create_word_indices_for_text(text_as_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "bfb1013d-2cd9-4ae7-a9c2-43c991e6d6ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_vectors = np.zeros((len(input_words), context, number_of_words), dtype=np.int16)\n",
    "vectorized_labels = np.zeros((len(input_words), number_of_words), dtype=np.int16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "1c45d275-fab5-4c39-a395-77f1737f5af6",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, input_w in enumerate(input_words):\n",
    "    for j, w in enumerate(input_w):\n",
    "        input_vectors[i, j, word2index[w]] = 1\n",
    "        vectorized_labels[i, word2index[label_words[i]]] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "37681059-afec-4084-b5e2-bdca7eeaadde",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(SimpleRNN(hidden_neurons, return_sequences=False, input_shape=(context, number_of_words), unroll=True))\n",
    "model.add(Dense(number_of_words))\n",
    "model.add(Activation(output_nonlinearity))\n",
    "model.compile(loss=error_function, optimizer=my_optimizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "1f63ef8f-d214-4fc0-baae-8de9ae0fa9b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">-<>-<>-<>-<>-<>-<>-<>-<>-<>-<>-<>-<>-<>-<>-<>-<>-<>-<>-<>-<>-<>-<>-<>-<>-<>-<>-<>-<>-<>-<>-<>-<>-<>-<>-<>-<>-<>-<>-<>-<>-<>-<>-<>-<>-<>-<>-<>-<>-<>-<\n",
      "Cycle: 1\n",
      "Epoch 1/3\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.0112  \n",
      "Epoch 2/3\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0112 \n",
      "Epoch 3/3\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.0112\n",
      "Generating test from test index 107 with words ['first', 'electric', 'car']:\n",
      "THE COMPLETE RESULTING SENTENCE IS: first electric car company\n",
      "\n",
      ">-<>-<>-<>-<>-<>-<>-<>-<>-<>-<>-<>-<>-<>-<>-<>-<>-<>-<>-<>-<>-<>-<>-<>-<>-<>-<>-<>-<>-<>-<>-<>-<>-<>-<>-<>-<>-<>-<>-<>-<>-<>-<>-<>-<>-<>-<>-<>-<>-<>-<\n",
      "Cycle: 2\n",
      "Epoch 1/3\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0112 \n",
      "Epoch 2/3\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0113 \n",
      "Epoch 3/3\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0112 \n",
      "Generating test from test index 114 with words ['units', 'globally.[6]', 'in']:\n",
      "THE COMPLETE RESULTING SENTENCE IS: units globally.[6] in as\n",
      "\n",
      ">-<>-<>-<>-<>-<>-<>-<>-<>-<>-<>-<>-<>-<>-<>-<>-<>-<>-<>-<>-<>-<>-<>-<>-<>-<>-<>-<>-<>-<>-<>-<>-<>-<>-<>-<>-<>-<>-<>-<>-<>-<>-<>-<>-<>-<>-<>-<>-<>-<>-<\n",
      "Cycle: 3\n",
      "Epoch 1/3\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0112 \n",
      "Epoch 2/3\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0112 \n",
      "Epoch 3/3\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0112 \n",
      "Generating test from test index 9 with words ['and', 'marc', 'tarpenning']:\n",
      "THE COMPLETE RESULTING SENTENCE IS: and marc tarpenning pickup\n",
      "\n",
      ">-<>-<>-<>-<>-<>-<>-<>-<>-<>-<>-<>-<>-<>-<>-<>-<>-<>-<>-<>-<>-<>-<>-<>-<>-<>-<>-<>-<>-<>-<>-<>-<>-<>-<>-<>-<>-<>-<>-<>-<>-<>-<>-<>-<>-<>-<>-<>-<>-<>-<\n",
      "Cycle: 4\n",
      "Epoch 1/3\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0112 \n",
      "Epoch 2/3\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0112 \n",
      "Epoch 3/3\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0112 \n",
      "Generating test from test index 108 with words ['electric', 'car', 'to']:\n",
      "THE COMPLETE RESULTING SENTENCE IS: electric car to roadster\n",
      "\n",
      ">-<>-<>-<>-<>-<>-<>-<>-<>-<>-<>-<>-<>-<>-<>-<>-<>-<>-<>-<>-<>-<>-<>-<>-<>-<>-<>-<>-<>-<>-<>-<>-<>-<>-<>-<>-<>-<>-<>-<>-<>-<>-<>-<>-<>-<>-<>-<>-<>-<>-<\n",
      "Cycle: 5\n",
      "Epoch 1/3\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.0113 \n",
      "Epoch 2/3\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0112 \n",
      "Epoch 3/3\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0112 \n",
      "Generating test from test index 120 with words ['y', 'was', 'the']:\n",
      "THE COMPLETE RESULTING SENTENCE IS: y was the the\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for cycle in range(cycles):\n",
    "    print(\">-<\" * 50)\n",
    "    print(\"Cycle: %d\" % (cycle+1))\n",
    "    model.fit(input_vectors, vectorized_labels, batch_size=batch_size, epochs=epochs_per_cycle)\n",
    "    test_index = np.random.randint(len(input_words))\n",
    "    test_words = input_words[test_index]\n",
    "    print(\"Generating test from test index %s with words %s:\" % (test_index, test_words))\n",
    "    input_for_test = np.zeros((1, context, number_of_words))\n",
    "    for i, w in enumerate(test_words):\n",
    "        input_for_test[0, i, word2index[w]] = 1\n",
    "    predictions_all_matrix = model.predict(input_for_test, verbose=0)[0]\n",
    "    predicted_word = index2word[np.argmax(predictions_all_matrix)]\n",
    "    print(\"THE COMPLETE RESULTING SENTENCE IS: %s %s\" % (\" \".join(test_words), predicted_word))\n",
    "    print()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
